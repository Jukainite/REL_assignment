{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import matplotlib\n",
    "import neptune\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "from itertools import product\n",
    "from matplotlib import pyplot\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.player.battle_order import ForfeitBattleOrder\n",
    "from poke_env.player.player import Player\n",
    "from scipy.interpolate import griddata\n",
    "from poke_env.player import Gen8EnvSinglePlayer, RandomPlayer,Gen9EnvSinglePlayer\n",
    "from poke_env.player import player as Sampleplayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "save_to_json_file = True\n",
    "use_validation = True\n",
    "use_neptune = False\n",
    "\n",
    "nest_asyncio.apply()\n",
    "np.random.seed(0)\n",
    "\n",
    "if use_neptune:\n",
    "    run = neptune.init_run(project='jukainite/pokeREL',\n",
    "                       api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2MTM2NzQ3NS0yODQ0LTRmNGItYWRmZi0yNjI1MDRiMDMxYjYifQ==',\n",
    "                       tags=[\"Q-learning FA deterministic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env.data import GenData\n",
    "GEN_9_DATA = GenData.from_gen(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxDamagePlayer(Player):\n",
    "    def choose_move(self, battle):\n",
    "        if battle.available_moves:\n",
    "            best_move = max(battle.available_moves, key=lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "        else:\n",
    "            return self.choose_random_move(battle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningFAPlayer(Player):\n",
    "    def __init__(self, battle_format, n0, alpha0, gamma):\n",
    "        # super().__init__(battle_format=battle_format, team=team)\n",
    "        super().__init__(battle_format=battle_format)\n",
    "        self.N = defaultdict(lambda: np.zeros(N_OUR_ACTIONS))\n",
    "        self.w = np.random.rand(N_FEATURES)  # Updated to match the feature vector size\n",
    "        self.n0 = n0\n",
    "        self.alpha0 = alpha0\n",
    "        self.gamma = gamma\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self._reward_buffer = {}\n",
    "\n",
    "    def choose_move(self, battle):\n",
    "        if self.state is not None:\n",
    "            # observe R, S'\n",
    "            reward = self.compute_reward(battle)\n",
    "            next_state = self.embed_battle(battle)\n",
    "            # Q-learning\n",
    "            self.N[str(self.state)][self.action] += 1\n",
    "            alpha = self.alpha0 / self.N[str(self.state)][self.action]\n",
    "            delta = (\n",
    "                reward + self.gamma * self.max_q_approx(next_state, self.w) - self.q_approx(self.state, self.action, self.w)\n",
    "            )\n",
    "            self.w += alpha * delta * self.x(self.state, self.action)\n",
    "            # S <- S'\n",
    "            self.state = next_state\n",
    "        else:\n",
    "            # S first initialization\n",
    "            self.state = self.embed_battle(battle)\n",
    "\n",
    "        # Choose A from S using epsilon-greedy policy\n",
    "        self.action = self.pi(self.state, self.w)\n",
    "\n",
    "        # if the selected action is not possible, perform a random move instead\n",
    "        if self.action == -1:\n",
    "            return ForfeitBattleOrder()\n",
    "        elif self.action < 4 and self.action < len(battle.available_moves) and not battle.force_switch:\n",
    "            return self.create_order(battle.available_moves[self.action])\n",
    "        elif 0 <= self.action - 4 < len(battle.available_switches):\n",
    "            return self.create_order(battle.available_switches[self.action - 4])\n",
    "        else:\n",
    "            return self.choose_random_move(battle)\n",
    "\n",
    "    def _battle_finished_callback(self, battle):\n",
    "        if use_neptune:\n",
    "            run[f'N0: {self.n0} gamma: {self.gamma} win_acc'].log(self.n_won_battles / len(self._reward_buffer))\n",
    "\n",
    "    ''' Helper functions '''\n",
    "\n",
    "    # feature vector\n",
    "    @staticmethod\n",
    "    def x(state, action):\n",
    "        state = np.array(state).astype(float)\n",
    "        return np.append(state, action)\n",
    "\n",
    "    # q^(S, A, W)\n",
    "    def q_approx(self, state, action, w):\n",
    "        \n",
    "        state = np.array(state).astype(float)\n",
    "        \n",
    "        return np.dot(self.x(state, action), w)\n",
    "\n",
    "    # max(a, q^(S, a', W))\n",
    "    def max_q_approx(self, state, w):\n",
    "        state = np.array(state).astype(float)\n",
    "        return max(np.array([self.q_approx(state, action, w) for action in range(N_OUR_ACTIONS)]))\n",
    "\n",
    "    # epsilon-greedy policy\n",
    "    def pi(self, state, w):\n",
    "        epsilon = self.n0 / (self.n0 + np.sum(self.N[str(state)]))\n",
    "        # let's get the greedy action. Ties must be broken arbitrarily\n",
    "        q_approx = np.array([self.q_approx(state, action, w) for action in range(N_OUR_ACTIONS)])\n",
    "        greedy_action = np.random.choice(np.where(q_approx == q_approx.max())[0])\n",
    "        action_pick_probability = np.full(N_OUR_ACTIONS, epsilon / N_OUR_ACTIONS)\n",
    "        action_pick_probability[greedy_action] += 1 - epsilon\n",
    "        return np.random.choice(ALL_OUR_ACTIONS, p=action_pick_probability)\n",
    "\n",
    "    # the embed battle is our state\n",
    "    # 10 factors: 4 moves base power, 4 moves multipliers, remaining mons\n",
    "    @staticmethod\n",
    "    def embed_battle(battle):\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                move.base_power / 100\n",
    "            )  # Simple rescaling to facilitate learning\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                    type_chart=GEN_9_DATA.type_chart\n",
    "                )\n",
    "\n",
    "        # We count how many pokemons have not fainted in each team\n",
    "        n_fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted])\n",
    "        n_fainted_mon_opponent = len([mon for mon in battle.opponent_team.values() if mon.fainted])\n",
    "\n",
    "        state = []\n",
    "        for move_base_power in moves_base_power:\n",
    "            state.append(move_base_power)\n",
    "        for move_dmg_multiplier in moves_dmg_multiplier:\n",
    "            state.append(move_dmg_multiplier)\n",
    "        state.append(n_fainted_mon_team)\n",
    "        state.append(n_fainted_mon_opponent)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    # Computing rewards\n",
    "    def reward_computing_helper(\n",
    "        self,\n",
    "        battle: AbstractBattle,\n",
    "        *,\n",
    "        fainted_value: float = 0.15,\n",
    "        hp_value: float = 0.15,\n",
    "        number_of_pokemons: int = 6,\n",
    "        starting_value: float = 0.0,\n",
    "        status_value: float = 0.15,\n",
    "        victory_value: float = 1.0\n",
    "    ) -> float:\n",
    "        # 1st compute\n",
    "        if battle not in self._reward_buffer:\n",
    "            self._reward_buffer[battle] = starting_value\n",
    "        current_value = 0\n",
    "\n",
    "        # Verify if pokemon have fainted or have status\n",
    "        for mon in battle.team.values():\n",
    "            current_value += mon.current_hp_fraction * hp_value\n",
    "            if mon.fainted:\n",
    "                current_value -= fainted_value\n",
    "            elif mon.status is not None:\n",
    "                current_value -= status_value\n",
    "\n",
    "        current_value += (number_of_pokemons - len(battle.team)) * hp_value\n",
    "\n",
    "        # Verify if opponent pokemon have fainted or have status\n",
    "        for mon in battle.opponent_team.values():\n",
    "            current_value -= mon.current_hp_fraction * hp_value\n",
    "            if mon.fainted:\n",
    "                current_value += fainted_value\n",
    "            elif mon.status is not None:\n",
    "                current_value += status_value\n",
    "\n",
    "        current_value -= (number_of_pokemons - len(battle.opponent_team)) * hp_value\n",
    "\n",
    "        # Verify if we won or lost\n",
    "        if battle.won:\n",
    "            current_value += victory_value\n",
    "        elif battle.lost:\n",
    "            current_value -= victory_value\n",
    "\n",
    "        # Value to return\n",
    "        to_return = current_value - self._reward_buffer[battle]\n",
    "        self._reward_buffer[battle] = current_value\n",
    "        if use_neptune:\n",
    "            run[f'N0: {self.n0} gamma: {self.gamma} reward_buffer'].log(current_value)\n",
    "        return to_return\n",
    "\n",
    "    # Calling reward_computing_helper\n",
    "    def compute_reward(self, battle) -> float:\n",
    "        return self.reward_computing_helper(battle, fainted_value=2, hp_value=1, victory_value=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "\n",
    "# possible values for num_battles (number of episodes)\n",
    "n_battles_array = [10000]\n",
    "# exploration schedule from MC, i. e., epsilon(t) = N0 / (N0 + N(S(t)))\n",
    "n0_array = [0.0001, 0.001, 0.01]\n",
    "# possible values for alpha0 (initial learning rate)\n",
    "alpha0_array = [0.01]\n",
    "# possible values for gamma (discount factor)\n",
    "gamma_array = [0.75]\n",
    "\n",
    "\n",
    "list_of_params = [\n",
    "    {\n",
    "        'n_battles': n_battles,\n",
    "        'n0': n0,\n",
    "        'alpha0': alpha0,\n",
    "        'gamma': gamma\n",
    "    } for n_battles, n0, alpha0, gamma in product(n_battles_array, n0_array, alpha0_array, gamma_array)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json helper functions\n",
    "\n",
    "def save_array_to_json(path_dir, filename, data):\n",
    "    if not os.path.exists(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "    full_filename = path_dir + \"/\" + filename\n",
    "    # write\n",
    "    with open(full_filename, \"w\") as file:\n",
    "        json.dump(data if isinstance(data, list) else data.tolist(), file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "def save_dict_to_json(path_dir, filename, data, append=True):\n",
    "    if not os.path.exists(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "    full_filename = path_dir + \"/\" + filename\n",
    "    if os.path.exists(full_filename) and append:\n",
    "        with open(full_filename, \"r\") as file:\n",
    "            value_dict = json.load(file)\n",
    "            for key in data:\n",
    "                value_dict[key] = data[key] if isinstance(data[key], list) else data[key].tolist()\n",
    "            file.close()\n",
    "    else:\n",
    "        value_dict = dict()\n",
    "        for key in data:\n",
    "            value_dict[key] = data[key] if isinstance(data[key], list) else data[key].tolist()\n",
    "    # write\n",
    "    with open(full_filename, \"w\") as file:\n",
    "        json.dump(value_dict, file)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "def read_array_from_json(path_dir, filename):\n",
    "    full_filename = path_dir + \"/\" + filename\n",
    "    if not os.path.exists(full_filename):\n",
    "        return None\n",
    "    file = open(full_filename, \"r\")\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_dict_from_json(path_dir, filename):\n",
    "    full_filename = path_dir + \"/\" + filename\n",
    "    if not os.path.exists(full_filename):\n",
    "        return None\n",
    "    file = open(full_filename, \"r\")\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main (let's battle!)\n",
    "\n",
    "# training\n",
    "async def do_battle_training():\n",
    "    for params in list_of_params:\n",
    "        start = time.time()\n",
    "        params['player'] = QLearningFAPlayer(battle_format=\"gen9randombattle\", n0=params['n0'], alpha0=params['alpha0'], gamma=params['gamma'])\n",
    "        params['opponent'] = MaxDamagePlayer(battle_format=\"gen9randombattle\", )\n",
    "        await params['player'].battle_against(opponent=params['opponent'], n_battles=params['n_battles'])\n",
    "        if debug:\n",
    "            print(\"training: num battles (episodes)=%d, N0=%.4f, alpha0=%.2f, gamma=%.2f, wins=%d, winning %%=%.2f, total time=%s sec\" %\n",
    "                  (\n",
    "                      params['n_battles'],\n",
    "                      round(params['n0'], 4),\n",
    "                      round(params['alpha0'], 2),\n",
    "                      round(params['gamma'], 2),\n",
    "                      params['player'].n_won_battles,\n",
    "                      round((params['player'].n_won_battles / params['n_battles']) * 100, 2),\n",
    "                      round(time.time() - start, 2)\n",
    "                  ))\n",
    "\n",
    "        # save w to json file\n",
    "        if save_to_json_file:\n",
    "            today_s = str(date.today())\n",
    "            n_battle_s = str(params['n_battles'])\n",
    "            n0_s = str(round(params['n0'], 4))\n",
    "            alpha0_s = str(round(params['alpha0'], 2))\n",
    "            gamma_s = str(round(params['gamma'], 2))\n",
    "            winning_percentage_s = str(round((params['player'].n_won_battles / params['n_battles']) * 100, 2))\n",
    "            filename = \"W_\" + today_s + \"_\" + n_battle_s + \"_\" + n0_s + \"_\" + alpha0_s + \"_\" + gamma_s + \"_\" + winning_percentage_s + \".json\"\n",
    "            save_array_to_json(\"./Q_Learning_FA_det_w\", filename, params['player'].w)\n",
    "\n",
    "        # statistics: key: \"n_battles, n0, alpha0, gamma\", values: list of win or lose\n",
    "        key = str(params['n_battles']) + \"_\" + str(round(params['n0'], 4)) + \"_\" + str(round(params['alpha0'], 2)) + \"_\" + str(round(params['gamma'], 2))\n",
    "        winning_status = list()\n",
    "        for battle in params['player']._battles.values():\n",
    "            if battle.won:\n",
    "                winning_status.append(True)\n",
    "            else:\n",
    "                winning_status.append(False)\n",
    "        # save statistics json file (append)\n",
    "        data = dict()\n",
    "        data[key] = winning_status\n",
    "        save_dict_to_json(\"./Q_Learning_FA_det_statistics\", \"statistics.json\", data)\n",
    "\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(loop.create_task(do_battle_training()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
